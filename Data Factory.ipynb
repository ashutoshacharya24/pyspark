{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a394bdb-93cd-4a73-9159-c560f3f8fc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Linked service and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7b31911-1ca3-4176-ac47-8b00fb11c1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### There is a migration work from source to destination, here source is having sql db and destination is having datalake/blobstorage then adf is the middleware in between source and destination. this is called linked service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d831f92-cbc2-48dd-beaf-938d87151c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### To establish a connection between storage account and ADF you go to ADF Studio --> manage tab -->Linked Service --> create new "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c34c31f0-838b-434e-a593-59464a831adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Copy Activity\n",
    "\n",
    "###### migrating data from source to destination(sink) is called copy activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d8e8fb-4fde-403d-99c4-02fbd18fcbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Get Metadata Activity\n",
    "\n",
    "###### It will give the metadata of the dataset folder. It will give you the all the file name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d57255-258b-4f70-a8f7-3d6667cabe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set Variable Activity\n",
    "\n",
    "###### By using this activity you can store a pipeline output value to use in other pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a11ba85e-1fcc-4fab-a07e-69a01a8b59ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Storage Events Trigger\n",
    "\n",
    "###### if your data is added at any particular time in the source and your pipeline should run once the file added at the source and dump it in the sink. and once it done it will stopped the pipeline and if any file added in source then it should run and dump data in sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d7821f2-da5b-4d8a-ba95-1b9a95539c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Execute Pipeline Activity\n",
    "###### You can use this pipeline for multiple pipeline execution one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa504eae-a454-41a8-bb1f-9c43eab4f870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# steps to create pipeline\n",
    "\n",
    "###### resource group\n",
    "###### storage account (blob/data lake)\n",
    "###### containers (folder structure creation)\n",
    "###### adf and launch the studio\n",
    "###### establish connection( source-->adf-->datalake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54830fd-16fe-4f90-9c8e-5cb6bac8160c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step to create service principle\n",
    "\n",
    "###### microsoft entra ID --> manage --> app registration --> create new registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83dead62-01aa-4dcc-8d89-5f7b04b8f93c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# recursiveFileLookup()\n",
    "\n",
    "###### it will read all the file directly whether u have hierarchical folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c051c241-cf0e-4d97-afe2-7d860313731d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Factory",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
